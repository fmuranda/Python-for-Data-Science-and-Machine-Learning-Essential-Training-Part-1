{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLTxqlr5230zAStKgBnUmW",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/fmuranda/Python-for-Data-Science-and-Machine-Learning-Essential-Training-Part-1/blob/main/Asynchronous_scraping.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install aiohttp\n",
        "!pip install asyncio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "K__60ZL_Z78c",
        "outputId": "1f158ae0-cf45-470a-e5bd-b03fc8226015"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.12/dist-packages (3.13.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.4.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.4.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (25.4.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.8.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (6.7.1)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (0.4.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.12/dist-packages (from aiohttp) (1.22.0)\n",
            "Requirement already satisfied: typing-extensions>=4.2 in /usr/local/lib/python3.12/dist-packages (from aiosignal>=1.4.0->aiohttp) (4.15.0)\n",
            "Requirement already satisfied: idna>=2.0 in /usr/local/lib/python3.12/dist-packages (from yarl<2.0,>=1.17.0->aiohttp) (3.11)\n",
            "Requirement already satisfied: asyncio in /usr/local/lib/python3.12/dist-packages (4.0.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "JudqbaIwZoBu"
      },
      "outputs": [],
      "source": [
        "import aiohttp\n",
        "import asyncio\n",
        "from bs4 import BeautifulSoup\n",
        "import csv\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! pip install nest_asyncio\n",
        "import nest_asyncio\n",
        "nest_asyncio.apply()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yduaYQU3aYZa",
        "outputId": "b2ac61d9-e33c-444d-8a76-f490b58484b4"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: nest_asyncio in /usr/local/lib/python3.12/dist-packages (1.6.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "async def scrap_and_save_links(text):\n",
        "  soup = BeautifulSoup(text, 'html.parser')\n",
        "  file = open('csv_file', 'a', newline =',')\n",
        "  writer = csv.writer(file, 'a',  newline =',')\n",
        "  for link in soup.findAll('a', attrs ={'href': re.compile('^http')}):\n",
        "    link  = link.get('href')\n",
        "    writer.writerow\n",
        "  file.close()"
      ],
      "metadata": {
        "id": "Jv4M2BA_ayh7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def fetch(session, url):\n",
        "  try:\n",
        "    async with session.get(url) as response:\n",
        "      text = await response.text()\n",
        "      task = asyncio.create_task(scrap_and_save_links(text))\n",
        "      await task\n",
        "  except Exception as e:\n",
        "    print(str(e))"
      ],
      "metadata": {
        "id": "Sd_YgSMYb3sc"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def scrap(urls):\n",
        "  tasks =[]\n",
        "  async with aiohttp.ClientSession() as session:\n",
        "    for url in urls:\n",
        "      tasks.append(fetch(session,url))\n",
        "    await asyncio.gather(*tasks)"
      ],
      "metadata": {
        "id": "74nBbfOXczkF"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "async def scrap_and_save_links(text):\n",
        "    soup = BeautifulSoup(text, 'html.parser')\n",
        "    # Using 'with' ensures the file is closed properly after writing\n",
        "    with open('links.csv', 'a', newline='') as file:\n",
        "        writer = csv.writer(file)\n",
        "        for link in soup.find_all('a', href=re.compile('^http')):\n",
        "            url = link.get('href')\n",
        "            if url:\n",
        "                writer.writerow([url])\n",
        "\n",
        "async def fetch(session, url):\n",
        "    try:\n",
        "        async with session.get(url, timeout=10) as response:\n",
        "            text = await response.text()\n",
        "            await scrap_and_save_links(text)\n",
        "    except Exception as e:\n",
        "        print(f\"Error fetching {url}: {e}\")\n",
        "\n",
        "async def scrap(urls):\n",
        "    async with aiohttp.ClientSession() as session:\n",
        "        tasks = [fetch(session, url) for url in urls]\n",
        "        await asyncio.gather(*tasks)\n",
        "\n",
        "urls = ['https://analytics.usa.gov', 'https://www.python.org', 'https://www.linkedin.com']\n",
        "await scrap(urls)"
      ],
      "metadata": {
        "id": "3ZmPBeKQefJj"
      },
      "execution_count": 33,
      "outputs": []
    }
  ]
}